{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "c1fe0899-8d5e-4a9b-b948-6bc10ebe8a11",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import chromadb\n",
                "from llama_index import VectorStoreIndex, ServiceContext\n",
                "from llama_index.vector_stores import ChromaVectorStore\n",
                "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
                "chroma_collection = db.get_or_create_collection(\"dagster-support\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "2c528129-096f-41dc-b2b0-9323970a10b8",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from llama_index.llms import \n",
                "from llama_index import VectorStoreIndex, ServiceContext, set_global_service_context\n",
                "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
                "from llama_index.response.notebook_utils import *\n",
                "\n",
                "embed_model = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
                "llm = OpenAI(model='gpt-3.5-turbo-16k', temperature=0, max_tokens=10000)\n",
                "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
                "set_global_service_context(service_context)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "1f2b5ad3-4f65-4457-ac61-12697b0d6c1a",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
                "index = VectorStoreIndex.from_vector_store(\n",
                "    vector_store,\n",
                "    service_context=service_context,\n",
                "    similarity_top_k=5,\n",
                "    max_top_k=20\n",
                ")\n",
                "query_engine = index.as_query_engine(service_context=service_context)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "997d6d63-1944-492b-9f2e-829b7ce089d0",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** I found a relevant Github issue that discusses the ability to pass custom config to a backfill in Dagster. The issue is titled \"Enable supplying config when launching a backfill\" and is currently open. You can find more information about it [here](https://github.com/dagster-io/dagster/issues/10079). \n",
                            "\n",
                            "Based on the discussion in the issue, it seems that there is currently no option in the UI to pass custom config to a backfill. However, one possible solution mentioned in the discussion is to override the run config of each partition manually. For example, instead of using the default config, you can specify custom config for each partition."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "response = query_engine.query(\"\"\"You are a support bot, searching for relevant information in Github Issues and Discussions. \n",
                "You will receive a question from the user, and will try to find relevant information. If you find a relevant result, include \n",
                "the type, name, and URL of the document, as well as a possible solution to their problem.\n",
                "\n",
                "The User's Question is: \n",
                "Hi, how can I pass custom config to a backfill? Currently there seems to be no such option in UI\n",
                "\"\"\")\n",
                "display_response(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "7e0b3943-ee18-4bf9-a10b-b649d03f8b7b",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** I found a relevant document that might help you with your question. The document is an open issue on GitHub titled \"Show asset / job / op code in Dagit\". You can find the document [here](https://github.com/dagster-io/dagster/issues/8939).\n",
                            "\n",
                            "In the document, the user suggests that it would be useful to be able to quickly inspect code without searching for images, git repos, or merge requests. They also mention that it would be helpful to have information like asset op_tags available. However, there is no response or solution provided in the document.\n",
                            "\n",
                            "I also found another open issue on GitHub titled \"How do I create a job with one asset and its dependencies (upstream)?\" You can find the document [here](https://github.com/dagster-io/dagster/issues/16015).\n",
                            "\n",
                            "In this document, the user mentions that the current documentation is too simple and only shows how to provide a list of assets to a job. They request an example in the documentation that teaches how to provide just one asset to a job and have it run that asset and all its dependencies upstream. Again, there is no response or solution provided in the document.\n",
                            "\n",
                            "Based on the information in these documents, it seems that there may be limitations or missing features in Dagit and the documentation regarding the display of asset, job, and op code. It is possible that the @op function and methods are not currently designed to show up in the Dagit UI or the dynamic config UI. Unfortunately, there is no specific solution or explanation provided in the documents.\n",
                            "\n",
                            "I would recommend engaging with the maintainers of the Dagster project by giving a thumbs-up reaction to the issues. This will help prioritize the issues and potentially get a response or solution from the maintainers."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "response = query_engine.query(\"\"\"You are a support bot, searching for relevant information in Github Issues and Discussions. \n",
                "You will receive a question from the user, and will try to find relevant information. If you find a relevant result, include \n",
                "the type, name, and URL of the document, as well as a possible solution to their problem.\n",
                "\n",
                "The User's Question is: \n",
                "Hello :wave:\n",
                "General question:\n",
                "I have an assets file with three basic assets defined with @asset decorator, and a basic @op and @job setup from the example in the docs.  I was curious why the @op function do_something doesn't show up in the Dagit UI?  Do they have to be attached to a graph or something logically simple?  When I run the launchpad and try to configure concurrency, the @op methods don't appear in the dynamic config UI so I'm assuming there must be a reason it doesn't show up?  There are no errors.\n",
                "TIA!\"\"\")\n",
                "display_response(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "id": "5960bba2-d2ca-4781-baa1-935836d4e8a8",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from llama_index.prompts import PromptTemplate\n",
                "\n",
                "template = (\n",
                "    \"We have provided Github Issues and Discussions as context below. \\n\"\n",
                "    \"---------------------\\n\"\n",
                "    \"{context_str}\"\n",
                "    \"\\n---------------------\\n\"\n",
                "     \"\"\"\n",
                "You are a support bot, searching for relevant information in Github Issues and Discussions.\n",
                "You will receive a question from the user, and will try to find relevant information.\n",
                "If you find a relevant result, include the type, name, and URL of the document, as well as a possible solution to their problem.\n",
                "An Example Answer:\n",
                "\n",
                "Summary: (summarize the question)\n",
                "Relevant Issues: (link to the issues or discussions)\n",
                "(Your Answer below)\n",
                "   \n",
                "Given this information, please answer the question: {query_str}\\n\"\"\"\n",
                ")\n",
                "qa_template = PromptTemplate(template)\n",
                "query_engine = index.as_query_engine(service_context=service_context, text_qa_template=qa_template)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "id": "e7f922eb-a09e-4de8-b963-745c869a8325",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: The user is asking why the @op function do_something does not show up in the Dagit UI and why it doesn't appear in the dynamic config UI when trying to configure concurrency.\n",
                            "\n",
                            "Relevant Issues:\n",
                            "- Title: Show asset / job / op code in Dagit\n",
                            "  Type: Issue\n",
                            "  URL: [https://github.com/dagster-io/dagster/issues/8939](https://github.com/dagster-io/dagster/issues/8939)\n",
                            "  Possible Solution: The issue suggests adding the ability to show asset/job/op code in Dagit. This feature would allow users to quickly inspect code without searching for images/git repos/merge requests and provide information otherwise unavailable like asset op_tags.\n",
                            "\n",
                            "- Title: Hooks in Software Designed Assets\n",
                            "  Type: Issue\n",
                            "  URL: [https://github.com/dagster-io/dagster/issues/14419](https://github.com/dagster-io/dagster/issues/14419)\n",
                            "  Possible Solution: The issue suggests adding hooks to either the dagster.Definition or the dagster.define_asset_job arguments. This would address the user's concern about the lack of hooks in software defined assets.\n",
                            "\n",
                            "(Your Answer below)\n",
                            "Based on the information from the relevant issues, it seems that the current version of Dagit may not show asset/job/op code and may not support hooks in software defined assets. These features are requested in the issues mentioned above. To address your specific concern about the @op function not showing up in the Dagit UI and the dynamic config UI, it is possible that this is a limitation of the current version. You may want to follow the progress of the mentioned issues and engage with the maintainers by giving them a üëç to express your interest in these features."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "question = '''Hello :wave:\n",
                "General question:\n",
                "I have an assets file with three basic assets defined with\n",
                "@asset decorator, and a basic @op and @job setup from the example in \n",
                "the docs.  I was curious why the @op function do_something doesn't show \n",
                "up in the Dagit UI?  Do they have to be attached to a graph or something \n",
                "logically simple?  When I run the launchpad and try to configure \n",
                "concurrency, the @op methods don't appear in the dynamic config UI so\n",
                "I'm assuming there must be a reason it doesn't show up?  There are no errors.\n",
                "TIA!'''\n",
                "\n",
                "response = query_engine.query(question)\n",
                "display_response(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "id": "68bda3a7-8a96-4741-9bc5-87226dcbd404",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: The user is asking for information on how to architect a pipeline in Dagster, specifically regarding the use of assets, IO managers, and ops.\n",
                            "\n",
                            "Relevant Discussions:\n",
                            "- Title: How do I do dynamic fan out with Dagster assets?\n",
                            "  - Category: Assets\n",
                            "  - URL: [https://github.com/dagster-io/dagster/discussions/15521](https://github.com/dagster-io/dagster/discussions/15521)\n",
                            "  - Answer: The discussion provides an example of achieving dynamic fan out in an asset using graph-backed assets and dynamic outputs. The code snippet demonstrates how to split a data frame into chunks, process each chunk in parallel, and return the results as a new asset. The example also includes a graph definition for the full enrichment process.\n",
                            "\n",
                            "Relevant Issues:\n",
                            "- Title: How can I avoid persisting large Spark dataframes between ops?\n",
                            "  - Category: How-to examples\n",
                            "  - URL: [https://github.com/dagster-io/dagster/discussions/6899](https://github.com/dagster-io/dagster/discussions/6899)\n",
                            "  - Answer: The discussion suggests using the `mem_io_manager` to store intermediate values in memory instead of on-disk. By using the `mem_io_manager` and the `in_process_executor`, ops can share the same process and avoid materializing the IO between steps. The discussion also mentions the option of using vanilla Python functions instead of splitting logic across multiple ops if checkpointing and re-execution are not necessary.\n",
                            "\n",
                            "Answer:\n",
                            "When architecting a pipeline in Dagster, there are different approaches you can take depending on your requirements. If you need to perform dynamic fan out with assets, you can refer to the discussion titled \"How do I do dynamic fan out with Dagster assets?\" [here](https://github.com/dagster-io/dagster/discussions/15521). The discussion provides an example code snippet that demonstrates how to split a data frame into chunks, process them in parallel, and return the results as a new asset.\n",
                            "\n",
                            "On the other hand, if you want to avoid persisting large Spark dataframes between ops, you can consider using the `mem_io_manager`. This allows you to store intermediate values in memory instead of on-disk. The discussion titled \"How can I avoid persisting large Spark dataframes between ops?\" [here](https://github.com/dagster-io/dagster/discussions/6899) provides more details on how to configure the `mem_io_manager` and the `in_process_executor` to achieve this.\n",
                            "\n",
                            "Ultimately, the choice between using assets, IO managers, or ops depends on your specific use case and requirements. Assets can be useful for managing data dependencies and providing a clear separation of concerns. IO managers like the `mem_io_manager` can help optimize performance by storing intermediate values in memory. Ops can be used for more fine-grained control and flexibility in your pipeline logic. Consider your needs and choose the approach that best fits your requirements."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "def ask_question(q):\n",
                "    response = query_engine.query(q)\n",
                "    display_response(response)\n",
                "    \n",
                "ask_question(''' Hey friends, new to Dagster, and am trying to track down any good articles/documents about how best to architect a pipeline \n",
                "             ‚Äî for example, I have an asset that generates a pandas DataFrame, then the next asset saves it to a database. But would it \n",
                "             be better to have an IO manager do that? But then feels like it isn‚Äôt as clear or explicit . . . so maybe an op?')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "id": "01bcc53b-8ef8-4f01-8598-5b4dea7ea9bc",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: How to iterate through values in an op to execute downstream ops for each value\n",
                            "\n",
                            "Relevant Discussions: [Ops in Parallel](https://github.com/dagster-io/dagster/discussions/14546)\n",
                            "\n",
                            "Answer: In order to iterate through values in an op and execute downstream ops for each value, you can use dynamic outputs in Dagster. \n",
                            "\n",
                            "Here's an example solution:\n",
                            "\n",
                            "```python\n",
                            "@op\n",
                            "def my_first_op() -> List[str]:\n",
                            "    return list_of_values\n",
                            "\n",
                            "@op\n",
                            "def my_second_op(value):\n",
                            "    # process value\n",
                            "\n",
                            "@graph\n",
                            "def process_values():\n",
                            "    values = my_first_op()\n",
                            "    for value in values:\n",
                            "        my_second_op(value)\n",
                            "```\n",
                            "\n",
                            "You can find more information and examples in the [Ops in Parallel](https://github.com/dagster-io/dagster/discussions/14546) discussion.\n",
                            "\n",
                            "Let me know if this helps!"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('''\n",
                "Hi everyone,probably a newbie question: how can I iterate through values in an op \n",
                "so it executes downstream ops for each value?  In my case I want to migrate data \n",
                "and I need to iterate through schemas names. In the original script it's done \n",
                "simply using for loops, but I feel a Dagster job needs another approach.\n",
                "''')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "bdfcae7d-8960-4d7a-9ff0-151f54587447",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: The user is asking how to select multiple sibling dbt models inside an AssetSelection when using the dbt_assets decorator for dagster asset creation.\n",
                            "\n",
                            "Relevant Discussions: \n",
                            "- Title: How to specify an upstream dependency for dbt-based assets\n",
                            "  Category: How-to examples\n",
                            "  URL: [https://github.com/dagster-io/dagster/discussions/13995](https://github.com/dagster-io/dagster/discussions/13995)\n",
                            "\n",
                            "Answer: In order to select multiple sibling dbt models inside an AssetSelection, you can use the `AssetKey` class to specify the asset keys for each model. The `AssetSelection` class allows you to select multiple assets based on their keys.\n",
                            "\n",
                            "Here's an example of how you can select 4 sibling dbt models inside an `AssetSelection`:\n",
                            "\n",
                            "```python\n",
                            "from dagster import AssetKey, AssetSelection\n",
                            "\n",
                            "asset_selection = AssetSelection(\n",
                            "    asset_keys=[\n",
                            "        AssetKey([\"dbt_model_1\"]),\n",
                            "        AssetKey([\"dbt_model_2\"]),\n",
                            "        AssetKey([\"dbt_model_3\"]),\n",
                            "        AssetKey([\"dbt_model_4\"]),\n",
                            "    ]\n",
                            ")\n",
                            "\n",
                            "# Use the asset_selection in your job definition\n",
                            "```\n",
                            "\n",
                            "Make sure to replace `\"dbt_model_1\"`, `\"dbt_model_2\"`, `\"dbt_model_3\"`, and `\"dbt_model_4\"` with the actual names of your dbt models.\n",
                            "\n",
                            "You can find more information about specifying upstream dependencies for dbt-based assets in the discussion [here](https://github.com/dagster-io/dagster/discussions/13995).\n",
                            "\n",
                            "I hope this helps! Let me know if you have any further questions."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('''So I have a dbt_assets decorator that is pulling in 4 dbt models. All siblings. But I can't seem to pull them together into a job since I switched to using the decorator for dagster asset creation. How would I select 4 siblings inside an AssetSelection?\n",
                "''')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "id": "8c616af0-a4bf-4e0f-9d66-98a444ade794",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: The user is asking if there is a way to provide a default configuration to an op inside a @graph_multi_asset.\n",
                            "\n",
                            "Relevant Discussions: \n",
                            "- Title: How to specify config for graph-backed assets or op-backed assets?\n",
                            "  - Category: Other\n",
                            "  - URL: [link](https://github.com/dagster-io/dagster/discussions/14572)\n",
                            "\n",
                            "Relevant Issue:\n",
                            "- Title: Creating multiple assets from the same configurable op using AssetsDefinition.from_op leads to confusing config scaffold\n",
                            "  - State: OPEN\n",
                            "  - URL: [link](https://github.com/dagster-io/dagster/issues/14198)\n",
                            "\n",
                            "Answer: Based on the provided information, it seems that there is currently no direct way to provide a default configuration to an op inside a @graph_multi_asset. However, a possible workaround is to create a graph_asset that uses the op, which provides a cleaner config structure. This can be achieved by defining a graph_asset that calls the op function. \n",
                            "\n",
                            "Example:\n",
                            "```python\n",
                            "@graph_asset\n",
                            "def my_graph_asset():\n",
                            "    return config_op()\n",
                            "```\n",
                            "\n",
                            "This approach will result in a config scaffold where the op is nested under the asset, providing a clearer relationship between the asset and the op.\n",
                            "\n",
                            "Please note that this workaround may not be ideal for all scenarios, and it is recommended to consider the specific requirements and limitations of your use case."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('''Is there a way to provide a default configuration to an op inside a @graph_multi_asset?''')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "id": "aea60b87-b8f3-4c71-8e23-62d03c5aaf4e",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: The user is asking if it is possible to disable the auto materialize policy for a specific asset in the Dagster project.\n",
                            "\n",
                            "Relevant Issues:\n",
                            "1. Title: Provide the ability to disable the `AssetMaterializePolicy` for individual assets\n",
                            "   Type: Issue\n",
                            "   URL: https://github.com/dagster-io/dagster/issues/15504\n",
                            "\n",
                            "2. Title: make it obvious when auto-materializing is off, but an asset has an auto-materialize policy\n",
                            "   Type: Issue\n",
                            "   URL: https://github.com/dagster-io/dagster/issues/13717\n",
                            "\n",
                            "Answer:\n",
                            "Based on the provided information, it seems that there is an open feature request (Issue #15504) to provide the ability to disable the `AssetMaterializePolicy` for individual assets in Dagster. However, there is no response or implementation ideas mentioned in the issue.\n",
                            "\n",
                            "It is recommended to follow the issue and engage with it by giving it a thumbs-up reaction to show your interest and potentially influence its prioritization.\n",
                            "\n",
                            "As of now, it is unclear if this feature will be implemented or if there are alternative ways to disable the auto materialize policy for specific assets. It would be best to monitor the issue and any related discussions for updates on this functionality."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('Is it possible to turn off auto materialize policy only for a specific asset?')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "id": "d5c61b74-ea10-4551-b183-4346e8c464a4",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: The user is asking if it is possible to define a retry policy/strategy on a graph asset and how to do it.\n",
                            "\n",
                            "Relevant Issues:\n",
                            "1. Issue: Allow retry_policy to be set on job/graph\n",
                            "   - State: CLOSED\n",
                            "   - URL: [https://github.com/dagster-io/dagster/issues/5553](https://github.com/dagster-io/dagster/issues/5553)\n",
                            "   - Labels: None\n",
                            "   - Votes: 1\n",
                            "\n",
                            "2. Issue: Configure retry strategy at instance level\n",
                            "   - State: OPEN\n",
                            "   - URL: [https://github.com/dagster-io/dagster/issues/10103](https://github.com/dagster-io/dagster/issues/10103)\n",
                            "   - Labels: type: feature-request\n",
                            "   - Votes: 1\n",
                            "\n",
                            "Answer:\n",
                            "Currently, there is no specific information available regarding defining a retry policy/strategy on a graph asset. However, there are two relevant issues that discuss retry policies at different levels:\n",
                            "\n",
                            "1. The closed issue [Allow retry_policy to be set on job/graph](https://github.com/dagster-io/dagster/issues/5553) suggests that the retry_policy parameter was missing in the `GraphDefinition.to_job` function. It seems that this issue has been resolved, but there is no further information on how to set the retry policy on a graph asset.\n",
                            "\n",
                            "2. The open issue [Configure retry strategy at instance level](https://github.com/dagster-io/dagster/issues/10103) discusses the possibility of configuring the retry strategy at the instance level. This would allow configuring the retry strategy on a per-deployment basis. However, this issue is still open and there is no implementation or solution provided yet.\n",
                            "\n",
                            "Based on the available information, it seems that defining a retry policy/strategy on a graph asset is not currently supported. You may consider following the open issue [Configure retry strategy at instance level](https://github.com/dagster-io/dagster/issues/10103) for updates on this feature request."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('Hi, is it possible to define retry policy/strategy on graph_asset? If yes, how?')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "id": "709e22ee-f471-4f50-a3f1-57f59c6fe1aa",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: How to delete branch deployments in Dagster Cloud?\n",
                            "\n",
                            "Relevant Discussions:\n",
                            "- Title: How do I delete Dagster Cloud branch deployments?\n",
                            "  Category: Cloud\n",
                            "  URL: [Discussion Link](https://github.com/dagster-io/dagster/discussions/16111)\n",
                            "\n",
                            "Relevant Answer:\n",
                            "To delete a branch deployment in Dagster Cloud, you have a few options:\n",
                            "1. Deleting branch deployments in the UI: As an organization admin, you can manage branch deployments in the Deployments tab of the cloud settings panel.\n",
                            "2. Deleting branch deployments programmatically: You can use the `dagster-cloud` CLI to programmatically delete a branch deployment. For example, you can run `dagster-cloud branch-deployment delete --deployment <deployment_id>`.\n",
                            "\n",
                            "Please note that branch deployments in Dagster Cloud are designed to automatically spin down when not in use, and they should hide in the Dagster deployment dropdown when the associated PR is closed or merged. However, if you want to completely delete a branch deployment, you can follow the above steps.\n",
                            "\n",
                            "Let me know if you need further assistance!"
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('Hello everyone, does any one know how can we delete partitions from the dagster cloud ?')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "id": "82542bf5-fa5f-462a-8b69-6628e850c97c",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: How to load assets produced by a run using the Python API if you have a run_id and the AssetKey.\n",
                            "\n",
                            "Relevant Discussions: \n",
                            "- [How do I find all the asset materializations for a run?](https://github.com/dagster-io/dagster/discussions/14613)\n",
                            "- [How do I load_asset_value from a job executed in process with mem_io_manager?](https://github.com/dagster-io/dagster/discussions/10781)\n",
                            "\n",
                            "Answer: To load assets produced by a run using the Python API, you can use the `get_assets_materializations_for_run` function provided in the first discussion. Here's an example of how to use it:\n",
                            "\n",
                            "```python\n",
                            "from dagster import DagsterInstance\n",
                            "\n",
                            "def load_assets(run_id, asset_key):\n",
                            "    instance = DagsterInstance.get()\n",
                            "    materializations = get_assets_materializations_for_run(instance, run_id)\n",
                            "    for materialization in materializations:\n",
                            "        if materialization.asset_key.to_string() == asset_key:\n",
                            "            # Access the materialized asset value\n",
                            "            asset_value = materialization.metadata_entry.value\n",
                            "            return asset_value\n",
                            "    return None\n",
                            "```\n",
                            "\n",
                            "You can call this function with the `run_id` and the `asset_key` you want to load. It will return the materialized asset value if found, or `None` if not found.\n",
                            "\n",
                            "Please note that you need to have the Dagster instance set up and running to use this function."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('''\n",
                "Hello, is there a way to load assets produced by a run using the Python API if I have a run_id,  and the AssetKey for the asset I want to materialize? This is how I‚Äôm currently materialising assets:\n",
                "# Materialize some asset\n",
                "result = dagster.materialize(assets=[...])\n",
                "\n",
                "# get the run id\n",
                "result.run_id\n",
                "''')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "id": "9f3a896f-7732-49cf-9ab6-f45a04501673",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: How to do a full-refresh for dbt assets on Dagster UI?\n",
                            "\n",
                            "Relevant Discussions: \n",
                            "- [How to trigger a full refresh of dbt incremental models with software-defined assets](https://github.com/dagster-io/dagster/discussions/8873)\n",
                            "\n",
                            "Answer: To trigger a full refresh of dbt incremental models using software-defined assets in Dagster, you can pass configuration to the underlying op that backs your software-defined assets. In the launchpad, you can include the configuration `full_refresh: true` for the op that backs your dbt software-defined assets. This will ensure that a full refresh occurs. Here is an example configuration:\n",
                            "\n",
                            "```yaml\n",
                            "ops:\n",
                            "  run_dbt_b83ee_a92a1:\n",
                            "    config:\n",
                            "      full_refresh: true\n",
                            "```\n",
                            "\n",
                            "Please note that the specific op name (`run_dbt_b83ee_a92a1`) may vary depending on your setup."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('Hi team! How can I do full-refresh for a dbt assets on Dagster UI? Not really sure how to use full_fresh under Config (see screenshot)')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "id": "b4bd8661-4377-45bc-94de-b66a082f7f7a",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: The user is asking if anyone has successfully persisted history logs to a duckdb file instead of sqlite.\n",
                            "\n",
                            "Relevant Issues: \n",
                            "- Title: sqlite event log watch problems on ephemeral instance\n",
                            "  Type: issue\n",
                            "  URL: https://github.com/dagster-io/dagster/issues/3665\n",
                            "\n",
                            "Answer: \n",
                            "Based on the provided information, there is no specific mention of persisting history logs to a duckdb file in the Github Issues and Discussions. However, the default storage for history logs in Dagster is sqlite. You can refer to the Dagster documentation for more information on the default sqlite storage: [Dagster Instance - SQLite Storage Default](https://docs.dagster.io/deployment/dagster-instance#sqlite-storage-default). If you want to use duckdb as the storage for history logs, you may need to explore custom storage options or extensions for Dagster."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('''Did anyone successfully persist history logs to a duckdb file, say instead of sqlite?\n",
                "https://docs.dagster.io/deployment/dagster-instance#sqlite-storage-default''')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "id": "20fdb81a-f22b-44ed-8094-c158225e9509",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: The user is asking if it is possible to run a \"dbt source freshness\" command with @dbt_assets, as there are no models associated when dbt runs a source freshness.\n",
                            "\n",
                            "Relevant Issues:\n",
                            "- Title: [dagster-dbt] Emit Outputs/AssetMaterialization events as the dbt command executes, rather than waiting for run_results.json\n",
                            "  - State: CLOSED\n",
                            "  - URL: https://github.com/dagster-io/dagster/issues/11752\n",
                            "  - Labels: type: feature-request,integration: dbt\n",
                            "\n",
                            "Answer:\n",
                            "Based on the information provided, it seems that running a \"dbt source freshness\" command with @dbt_assets may not be possible because there are no models associated when dbt runs a source freshness. However, there is a closed issue on the Dagster GitHub repository that suggests implementing a new method to execute dbt commands in a streaming manner, which could potentially allow for real-time detection of materialized models during the source freshness command. You can find more information about this feature request in the closed issue: [dagster-dbt] Emit Outputs/AssetMaterialization events as the dbt command executes, rather than waiting for run_results.json.\n",
                            "\n",
                            "Please note that this feature request may not be available in the current version of Dagster, so you may need to check the latest updates or consult the Dagster documentation for more information on running a \"dbt source freshness\" command with @dbt_assets."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('''\n",
                "Hello ... How do you run a \"dbt source freshness\" with @dbt_assets ?\n",
                "There are no models associated (to be materialized) when dbt runs a source freshness so is this even possible? I.e. something like below.\n",
                "My first attempt was just putting together a function that only ran a source freshness (the manifest.json has one model in it)\n",
                "@dbt_assets(\n",
                "    manifest=dbt_manifest_path,\n",
                "    dagster_dbt_translator=CustomDagsterDbtTranslator(),\n",
                "    io_manager_key=\"snowflake_io_manager\",\n",
                ")\n",
                "def my_dbt_assets(context: OpExecutionContext, dbt: DbtCliResource):\n",
                "    yield from dbt.cli([\"source\",\"freshness\"], context=context).stream()\n",
                "Running dbt command: `dbt --no-write-json --no-use-colors source freshness --target dev --select fqn:the_models_@dbt_assets`.\n",
                "''')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "id": "58ba0fdd-1e4a-494a-a9ef-6f72d7cdcd7f",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: The user is asking if it is possible to kick off a local Dagster job within a Python program and still use Python multiprocessing.\n",
                            "\n",
                            "Relevant Discussions:\n",
                            "- Title: \"Execute job in multiprocessing in Python\"\n",
                            "  - Category: How-to examples\n",
                            "  - URL: [https://github.com/dagster-io/dagster/discussions/8467](https://github.com/dagster-io/dagster/discussions/8467)\n",
                            "\n",
                            "Answer:\n",
                            "Yes, it is possible to execute a job with multiprocessing in Python using Dagster. The user can use the `execute_job` API and wrap the job using `reconstructable` or `build_reconstructable_job`. They can also call `my_job.execute_in_process()` to run the job sequentially in process.\n",
                            "\n",
                            "Possible Solution:\n",
                            "```python\n",
                            "from dagster import execute_job, reconstructable, DagsterInstance\n",
                            "\n",
                            "result = execute_job(\n",
                            "    reconstructable(my_job_name),\n",
                            "    instance=DagsterInstance.get(),\n",
                            ")\n",
                            "```\n",
                            "\n",
                            "More details can be found in the [documentation](https://docs.dagster.io/overview/execution-model#multiprocessing).\n",
                            "\n",
                            "Please note that this answer is based on the provided context and may not cover all possible scenarios."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('Is it possible to chain together jobs in dagster?')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "id": "0be2a032-713f-4b93-bc6a-eba71634b7c1",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: The user is asking if there is a methodology or best practice for creating parameterized assets and asset graphs. They have a scenario where they need to call an API multiple times and want to declare the returned data individually for auditing. They are wondering if they have to manually create each pipeline or if they can be autogenerated.\n",
                            "\n",
                            "Relevant Discussions: \n",
                            "- Title: Programmatically generating graph-based assets\n",
                            "  Category: Other\n",
                            "  URL: [Github Discussion](https://github.com/dagster-io/dagster/discussions/15308)\n",
                            "\n",
                            "Relevant Answer:\n",
                            "In the provided discussion, the user is trying to programmatically generate a graph-based asset. They are using the `AssetsDefinition.from_graph` method to define the asset, but they are encountering an exception. Another user suggests removing the `@graph_asset` line as it is redundant with `AssetsDefinition.from_graph`. This may help resolve the issue.\n",
                            "\n",
                            "Possible Solution:\n",
                            "To create parameterized assets and asset graphs, you can define your assets using functions and decorators. You can use the `@asset` decorator to define individual assets and the `@graph` decorator to define a graph-based asset. By using functions and decorators, you can programmatically generate assets based on external data. Additionally, you can use the `ins` parameter in the `@asset` decorator to specify the upstream assets that an asset depends on. This allows you to collect and return the outputs of multiple other assets. \n",
                            "\n",
                            "Example:\n",
                            "```python\n",
                            "from dagster import asset, graph\n",
                            "\n",
                            "@asset\n",
                            "def asset_a1():\n",
                            "    # Define asset_a1 logic\n",
                            "    return data_a1\n",
                            "\n",
                            "@asset\n",
                            "def asset_a2():\n",
                            "    # Define asset_a2 logic\n",
                            "    return data_a2\n",
                            "\n",
                            "@asset\n",
                            "def asset_a3():\n",
                            "    # Define asset_a3 logic\n",
                            "    return data_a3\n",
                            "\n",
                            "@asset\n",
                            "def asset_combined(inputs):\n",
                            "    # Define asset_combined logic using inputs from upstream assets\n",
                            "    return combined_data\n",
                            "\n",
                            "@graph\n",
                            "def asset_pipeline():\n",
                            "    asset_combined(asset_a1())\n",
                            "    asset_combined(asset_a2())\n",
                            "    asset_combined(asset_a3())\n",
                            "```\n",
                            "\n",
                            "In this example, `asset_a1`, `asset_a2`, and `asset_a3` are individual assets, and `asset_combined` is a graph-based asset that depends on the outputs of the individual assets. By defining the assets and their dependencies in a function, you can easily generate the assets based on external data.\n",
                            "\n",
                            "Please note that this is just one possible approach, and there may be other ways to achieve your desired result. It's recommended to refer to the Dagster documentation and explore different options based on your specific use case."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('''\n",
                "Hello everyone, I am just wondering if there is a methodology/best practice\n",
                "around creating parameterized assets and asset graphs? I have an a scenario where \n",
                "I'm essentially having to call an API multiple times but want to declare the returned data individually for auditing. The data pipeline is the same for each so was wondering if I have to manually create each of these pipelines or if they can be autogenerated?\n",
                "asset-a1 -> asset-a1-processing -> asset-combined\n",
                "asset-a2 -> asset-a2-processing -> asset-combined\n",
                "asset-a3 -> asset-a3-processing -> asset-combined (ed\n",
                "''')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "id": "5bc93848-374a-4cd5-a006-9d10475a93ab",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "**`Final Response:`** Summary: The user is asking if there is a way to check if an asset run was triggered as part of a backfill.\n",
                            "\n",
                            "Relevant Discussions: [How do you check whether a run was part of a backfill?](https://github.com/dagster-io/dagster/discussions/16189)\n",
                            "\n",
                            "Answer: Yes, runs that were triggered as part of a backfill will be assigned a backfill tag that includes the backfill ID. You can check if an asset run was triggered as part of a backfill by inspecting the tags attribute of the DagsterRun object or by using the DagsterInstance.get_run_tags method."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ask_question('Hi. Is there an option to check whether an asset run was triggered as part of a backfill?')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "63bf09cd-180c-42cd-a6d5-164d16502e9f",
            "metadata": {},
            "outputs": [],
            "source": [
                "ask_question('"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
